{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import cPickle\n",
    "import patsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators support arguments to control the fitting behaviour -- these arguments are often called hyperparameters. Among the most important ones for GBRT are:\n",
    "\n",
    "* number of regression trees (n_estimators)\n",
    "* depth of each individual tree (max_depth)\n",
    "* loss function (loss)\n",
    "* learning rate (learning_rate)\n",
    "\n",
    "For example if you want to fit a regression model with 100 trees of depth 3 using least-squares:\n",
    "\n",
    "###[GBDT（MART）概念简介](http://www.cnblogs.com/downtjs/p/3286165.html)\n",
    "\n",
    "![](http://img.blog.csdn.net/20140502093849718)\n",
    "\n",
    "目前GBDT有两个不同的描述版本，两者各有支持者，读文献时要注意区分。残差版本把GBDT说成一个残差迭代树，认为每一棵回归树都在学习前N-1棵树的残差，之前我写的GBDT入门教程主要在描述这一版本，ELF开源软件实现中用的也是这一版本。Gradient版本把GBDT说成一个梯度迭代树，使用梯度下降法求解，认为每一棵回归树在学习前N-1棵树的梯度下降值，之前leftnoteasy的博客中介绍的为此版本，umass的源码实现中用的则是这一版本（准确的说是LambdaMART中的MART为这一版本，MART实现则是前一版本）。\n",
    "\n",
    " \n",
    "\n",
    "对GBDT无基础的朋友可以先分别看一下前面两篇博文教程。总的来说两者相同之处在于，都是迭代回归树，都是累加每颗树结果作为最终结果（Multiple Additive Regression Tree)，每棵树都在学习前N-1棵树尚存的不足，从总体流程和输入输出上两者是没有区别的；两者的不同主要在于每步迭代时，是否使用Gradient作为求解方法。前者不用Gradient而是用残差----残差是全局最优值，Gradient是局部最优方向*步长，即前者每一步都在试图让结果变成最好，后者则每步试图让结果更好一点。\n",
    "\n",
    " \n",
    "\n",
    "两者优缺点。看起来前者更科学一点--有绝对最优方向不学，为什么舍近求远去估计一个局部最优方向呢？原因在于灵活性。前者最大问题是，由于它依赖残差，cost function一般固定为反映残差的均方差，因此很难处理纯回归问题之外的问题。而后者求解方法为梯度下降，只要可求导的cost function都可以使用，所以用于排序的LambdaMART就是用的后者。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Hyperparameter tuning\n",
    "\n",
    "太耗时间了 12h+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X = train_data[0:, 1:]\n",
    "# y = train_data[0:, 0]\n",
    "\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.1, 0.05],\n",
    "#     'max_depth': [4, 6],\n",
    "#     'min_samples_leaf': [3, 9, 17],\n",
    "#     #'max_features': [1.0, 0.3, 0.1]\n",
    "# }\n",
    "# est = GradientBoostingClassifier(n_estimators=500)\n",
    "# gs_cv = GridSearchCV(est, param_grid).fit(X, y)\n",
    "# # best hyperparameter setting\n",
    "# gs_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('data/train_set/1212train_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_data = train_set.values\n",
    "# formula = 'buy ~ f3_1 + f3_2 + f3_3 + f3_4 + \\\n",
    "#                  f3_5 + f3_6 + f3_7 + f3_8 + \\\n",
    "#                  f1_1_3 + f1_1_7 + f1_2_3 + f1_2_7 + f1_3_3 + f1_3_7 + \\\n",
    "#                  f2_1 + np.true_divide(f3_4, f2_1+0.01)'# + np.true_divide(f2_1, f2_2)'\n",
    "formula = 'buy ~ f3_1 + f3_2 + f3_3 + f3_4 + \\\n",
    "                 f3_5 + f3_6 + f3_7 + f3_8 + \\\n",
    "                 f1_1_3 + f1_1_7 + f1_2_3 + f1_2_7 + f1_3_3 + f1_3_7 + \\\n",
    "                 f2_1 + np.true_divide(f3_4, f2_1+0.01)'# + np.true_divide(f2_1, f2_2)'\n",
    "# 用patsy的dmatrices生成一个对 友好的dataframe\n",
    "y, x = patsy.dmatrices(formula, data=train_set, return_type='dataframe')\n",
    "\n",
    "del train_set\n",
    "\n",
    "x = x.values[0:, 1:]\n",
    "y = y.values[0:, 0]\n",
    "\n",
    "# # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "# with open(\"models/f16_gbrt 1212[{'n_estimators': 200, 'loss': \\\n",
    "# 'exponential', 'learning_rate': 0.02, 'max_depth': 10, 'min_samples_leaf': 4}].pickle\") as f:\n",
    "#     gbrt_200 = cPickle.load(f)\n",
    "    \n",
    "# models = {\n",
    "#     gbrt_200: 200,\n",
    "# }\n",
    "\n",
    "# 对于gbrt而言，先固定树的数量后，根据3倍步长的经验遍历一下其它参数就很容易找到最优解，再根据线上的实际效果增加数的数量\n",
    "# 调参数\n",
    "params = {\n",
    "    'init': None, #gbrt_200,\n",
    "    #'loss': 'exponential',\n",
    "    'n_estimators': 700,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'learning_rate': 0.1,\n",
    "}\n",
    "\n",
    "est = GradientBoostingClassifier(\n",
    "    #init             = params['init'],\n",
    "    #loss             = params['loss'],\n",
    "    n_estimators     = params['n_estimators'],\n",
    "    max_depth        = params['max_depth'],\n",
    "    min_samples_leaf = params['min_samples_leaf'],\n",
    "    learning_rate    = params['learning_rate']\n",
    ")\n",
    "est.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mparams = dict(params)\n",
    "#mparams['n_estimators'] += models[mparams.pop('init')]\n",
    "mparams.pop('init')\n",
    "with open('models/f16_gbrt 1212[%s].pickle'%repr(mparams), 'w') as f:\n",
    "    cPickle.dump(est, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('data/test_set/1212test_set.csv')\n",
    "#test_data = test_set.values\n",
    "test_set['buy'] = 0\n",
    "\n",
    "formula = 'buy ~ f3_1 + f3_2 + f3_3 + f3_4 + \\\n",
    "                 f3_5 + f3_6 + f3_7 + f3_8 + \\\n",
    "                 f1_1_3 + f1_1_7 + f1_2_3 + f1_2_7 + f1_3_3 + f1_3_7 + \\\n",
    "                 f2_1 + np.true_divide(f3_4, f2_1+0.01)'# + np.true_divide(f2_1, f2_2)'\n",
    "# 用patsy的dmatrices生成一个对 友好的dataframe\n",
    "y_test, x_test = patsy.dmatrices(formula, data=test_set, return_type='dataframe')\n",
    "\n",
    "x_test = x_test.values[:, 1:]\n",
    "y_test = y_test.values[:, 0]\n",
    "\n",
    "#output = est.predict(test_data[0:, 2:])\n",
    "output_prob = est.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_set = pd.read_csv(\"data/test_set/1212ui_set.csv\")\n",
    "\n",
    "print 'predicting ...'\n",
    "predict_set['buy'] = output_prob[0:,1]\n",
    "print 'predict done'\n",
    "\n",
    "predict_set.to_csv(\n",
    "    \"data/output/gbrt/f16_predict_set 1212 %d %d.csv\"%(\n",
    "        params['n_estimators'], #+models[params['init']],\n",
    "        params['max_depth']\n",
    "#        params['min_samples_leaf'],\n",
    "#        params['loss'],\n",
    "#        params['learning_rate']\n",
    "    ),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict_set = pd.read_csv('data/output/gbrt/predict_set.csv')\n",
    "\n",
    "# recomm_set = predict_set.sort(columns=['buy'], ascending=False)[:421][['user_id','item_id']]\n",
    "# recomm_set.to_csv(\"data/output/gbrt/tianchi_mobile_recommendation_predict.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
