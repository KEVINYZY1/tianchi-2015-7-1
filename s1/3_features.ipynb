{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#特征工程\n",
    "\n",
    "生成特征：一个最基础的特征组合包括UI特征，U特征，I特征；特征的具体定义就是发挥你智慧的空间了\n",
    "\n",
    "\n",
    "##I 品牌的特征 f1_xxx（哪些品牌会被买？）\n",
    "\n",
    "##U 用户的特征 f2_xxx（哪些用户会买？） \n",
    "\n",
    "##UI 用户品牌的特征 f3_xxx（用户会买哪些品牌？）\n",
    "\n",
    "###基本统计类特征\n",
    "\n",
    "1. 截止到最后一天（比如交叉验证训练集的第61天）累计的点击（或衰减后点击）\n",
    "2. 同样的特征分别再针对收藏、购物车、购买等操作进行统计\n",
    "3. 截止到最后一天，用户对品牌进行过访问的次数、购买的次数、收藏的次数、购物车的次数（或经过时间衰减）\n",
    "4. 截止到最后一天，用户对品牌进行第一次访问（收藏、购物车、购买）到最后一次访问的时间间隔（或经过时间衰减）\n",
    "5. 用户最后一次访问（收藏、购物车、购买）距离分隔日期（如62）的时间间隔\n",
    "    \n",
    "###比值类特征\n",
    "\n",
    "1. 购买该品牌次数/总购买次数\n",
    "2. 对该品牌的总点击（购买、收藏、购物车）数/访问次数\n",
    "3. 在对品牌A进行访问的那些天里，用户对A的点击数/那些天里的总点击数\n",
    "4. 在对品牌A进行访问的那些天里，A的销量/那些天里的所有品牌的总销量（这个特征在实现思路上消耗内存太大，未能实现）##用户品牌的特征（用户会买哪些品牌？）\n",
    "5. 用户访问品牌A的天数/用户总活跃天数\n",
    "6. 用户购买品牌A的天数/用户总购买天数\n",
    "\n",
    "\n",
    "用【用户购买该brand的天数/用户访问该brand的天数】来刻画用户对该品牌的忠诚度\n",
    "\n",
    "用户购买品牌时的月份即用字符串表示，而用户购买品牌的次数则用数字表示\n",
    "\n",
    "可以发现点击量越高的，点击转购买率也往往越低，尤其点击量极高的用户，往往购买量为0。由此可以考虑将用户购买量、点击转购买率等一系列特征用于描述用户是否会在未来一个月发生购买\n",
    "\n",
    "统计 用户从初次访问品牌到最终购买品牌的时间，可以发现绝大多数购买都是当天接触当天完成，越往后用户购买的可能性越低。由此可以推测用户对品牌的购买意愿是随着距离上次访问的时间拉长而衰减的，进而可以使用衰减函数来模拟该购买意愿。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Unique (U,I) index in train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_features = {\n",
    "    'f_behavior_count': {1:'f3_1', 2:'f3_2', 3:'f3_3', 4:'f3_4'},\n",
    "    'f_behavior_lasttime_delta': {1:'f3_5', 2:'f3_6', 3:'f3_7', 4:'f3_8'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. UI特征 （用户-商品特征）\n",
    "\n",
    "###1 基本统计类特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parseDate = lambda s: datetime.datetime.strptime(s, \"%Y-%m-%d %H\")\n",
    "def f_behavior_count(df):\n",
    "    '''截止到最后一天，用户对商品进行过访问的次数、购买的次数、收藏的次数、购物车的次数\n",
    "    TODO: 加时间衰减\n",
    "    @return dict; {'f3_1':browse_count, 'f3_2': collect_count, ...}\n",
    "    '''\n",
    "    features = {1:'f3_1', 2:'f3_2', 3:'f3_3', 4:'f3_4'}\n",
    "    c = df.groupby('behavior_type').count()\n",
    "    f = defaultdict(int)\n",
    "    for b in c.index:\n",
    "        f[features[b]] = c.loc[b][0]\n",
    "    return [f['f3_1'],f['f3_2'], f['f3_3'], f['f3_4']]\n",
    "\n",
    "def f_behavior_lasttime_delta(df, seperator=datetime.datetime(2014,12,18,0)):\n",
    "    '''用户最后一次对商品访问（收藏、购物车、购买）距离分隔日期（如62）的时间间隔，如果没有发生动作，默认间隔60天\n",
    "    @return dict; \n",
    "    '''\n",
    "    features = {1:'f3_5', 2:'f3_6', 3:'f3_7', 4:'f3_8'}\n",
    "    group_max = df.groupby('behavior_type').max()\n",
    "    f = defaultdict(int)\n",
    "    for v in features.values(): f[v] = 60\n",
    "    for b in group_max.index:\n",
    "        f[features[b]] = (seperator - parseDate(group_max.loc[b].time)).days\n",
    "    return [f['f3_5'],f['f3_6'], f['f3_7'], f['f3_8']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2 比值类特征\n",
    "\n",
    "1. 购买该品牌次数/总购买次数\n",
    "2. 对该品牌的总点击（购买、收藏、购物车）数/访问次数\n",
    "3. 在对品牌A进行访问的那些天里，用户对A的点击数/那些天里的总点击数\n",
    "4. 在对品牌A进行访问的那些天里，A的销量/那些天里的所有品牌的总销量（这个特征在实现思路上消耗内存太大，未能实现）##用户品牌的特征（用户会买哪些品牌？）\n",
    "5. 用户访问品牌A的天数/用户总活跃天数\n",
    "6. 用户购买品牌A的天数/用户总购买天数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##构建train_set （训练集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# raw_train_set = pd.read_csv('data/raw_train_set.csv')\n",
    "# gp_ui = raw_train_set.groupby(['user_id', 'item_id'])\n",
    "\n",
    "# columns = ['user_id', 'item_id']\n",
    "# columns.extend(_features['f_behavior_count'].values())\n",
    "# columns.extend(_features['f_behavior_lasttime_delta'].values())\n",
    "\n",
    "# train_set = pd.DataFrame(columns=columns)\n",
    "# for (u,i), gpdf in gp_ui:\n",
    "#     row = [u,i]\n",
    "#     row.extend(f_behavior_count(gpdf))\n",
    "#     row.extend(f_behavior_lasttime_delta(gpdf))\n",
    "#     train_set.loc[len(train_set)] = row\n",
    "\n",
    "# train_set.to_csv('data/train_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Get train UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_UI(train='data/raw_train_set.csv', to_file='data/train_set/ui_set.csv'): \n",
    "    raw_train_set = pd.read_csv(train)\n",
    "    train_set = raw_train_set[['user_id', 'item_id']]\n",
    "\n",
    "    gp_ui = train_set.groupby(['user_id', 'item_id'])\n",
    "    ui_list = ( (u,i) for (u,i),gpdf in gp_ui)\n",
    "    pd.DataFrame(ui_list, columns=['user_id', 'item_id']).to_csv(to_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File data/1212raw_train_set.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7f44ec815f83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 1212\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mget_train_UI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/1212raw_train_set.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/train_set/1212ui_set.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# # ~1212\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# get_train_UI()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-49993a2cf178>\u001b[0m in \u001b[0;36mget_train_UI\u001b[1;34m(train, to_file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_train_UI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/raw_train_set.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/train_set/ui_set.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mraw_train_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_train_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mgp_ui\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuroro/.virtualenvs/ml/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    468\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuroro/.virtualenvs/ml/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuroro/.virtualenvs/ml/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    560\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuroro/.virtualenvs/ml/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    697\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuroro/.virtualenvs/ml/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1064\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1066\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3163)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5779)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File data/1212raw_train_set.csv does not exist"
     ]
    }
   ],
   "source": [
    "# 1212\n",
    "get_train_UI('data/1212raw_train_set.csv', 'data/train_set/1212ui_set.csv')\n",
    "# # ~1212\n",
    "# get_train_UI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##将训练集打上标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag(df, totag, fromtag, to_file='data/train_set.csv'):\n",
    "    '''将训练集用result打上buy标记'''\n",
    "    result_set = set((i[0],i[1]) for i in fromtag.values)\n",
    "    #train_set = pd.DataFrame(train_set, \n",
    "    #   columns=['user_id', 'item_id', 'behavior_type', 'user_geohash', 'item_category', 'time', 'buy'])\n",
    "    print 'to tag'\n",
    "    df['buy'] = [ 1 if ui in result_set else 0 \n",
    "        for ui, gpdf in totag\n",
    "    ]\n",
    "    if to_file:\n",
    "        df.to_csv(to_file, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tag_of_train_set():\n",
    "    '''get train_set/tag_set.csv'''\n",
    "    result = pd.read_csv('data/result.csv')\n",
    "    raw_train_set = pd.read_csv('data/1212raw_train_set.csv')\n",
    "    train_set = raw_train_set[['user_id', 'item_id']]\n",
    "    print 'result, train_set loaded'\n",
    "\n",
    "    gp_ui = train_set.groupby(['user_id', 'item_id'])\n",
    "    print 'groupby ok'\n",
    "\n",
    "    columns = ['buy']\n",
    "    tag_set = pd.DataFrame(columns=columns)\n",
    "\n",
    "    tag(tag_set, gp_ui, result, to_file='data/train_set/1212tag_set.csv')\n",
    "    print 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Merge train features to train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def merge_features_to_train_set():\n",
    "#     df_tag = pd.read_csv('data/train_set/tag_set.csv')\n",
    "#     df_f3_1to4 = pd.read_csv('data/train_set/feature_set_behavior_count.csv')\n",
    "#     df_f3_5to8 = pd.read_csv('data/train_set/feature_set_behavior_lasttime_delta.csv')\n",
    "#     df_f2_1to2 = pd.read_csv('data/train_set/feature_u_count.csv')\n",
    "#     pd.concat([df_tag, df_f3_1to4, df_f3_5to8, df_f2_1to2], axis=1)\\\n",
    "#     .to_csv('data/train_set/train_set.csv', index=False)\n",
    "    \n",
    "def merge_features_to_train_set1212():\n",
    "#     df_tag = pd.read_csv('data/train_set/1212tag_set.csv')\n",
    "#     df_f3_1to4 = pd.read_csv('data/train_set/1212feature_set_behavior_count.csv')\n",
    "#     df_f3_5to8 = pd.read_csv('data/train_set/1212feature_set_behavior_lasttime_delta.csv')\n",
    "#     df_f2_1to2 = pd.read_csv('data/train_set/1212feature_u_count.csv')\n",
    "#     df_f1 = pd.read_csv('data/train_set/1212feature_i_count.csv')\n",
    "    pd.concat([\n",
    "            pd.read_csv('data/train_set/1212tag_set.csv'),\n",
    "            pd.read_csv('data/train_set/1212feature_set_behavior_count.csv'),\n",
    "            pd.read_csv('data/train_set/1212feature_set_behavior_lasttime_delta.csv'),\n",
    "            pd.read_csv('data/train_set/1212feature_u_count.csv'),\n",
    "            pd.read_csv('data/train_set/1212feature_i_count.csv'),\n",
    "        ], axis=1).to_csv('data/train_set/1212train_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO 分1212\n",
    "tag_of_train_set()\n",
    "\n",
    "## 1212\n",
    "#merge_features_to_train_set1212()\n",
    "\n",
    "# ～1212\n",
    "# merge_features_to_train_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##构建test_set (测试集)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Get test UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_UI():    \n",
    "    test_set = pd.read_csv('data/test_set/tianchi_mobile_recommend_test_user.csv')\n",
    "    test_set = test_set[['user_id', 'item_id']]\n",
    "    print 'test set loaded'\n",
    "    # P商品子集\n",
    "    P = pd.read_csv('data/raw/tianchi_mobile_recommend_train_item.csv')\n",
    "    P_item_set = set(P.item_id.values)\n",
    "    # 过滤出 item_id 属于 P(商品子集) 的记录\n",
    "    test_set = test_set[test_set.item_id.map(lambda x: x in P_item_set)]\n",
    "\n",
    "    gp_ui = test_set.groupby(['user_id', 'item_id'])\n",
    "    ui_list = ( (u,i) for (u,i),gpdf in gp_ui)\n",
    "    pd.DataFrame(ui_list, columns=['user_id', 'item_id']).to_csv('data/test_set/1212ui_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Merge test features to test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def merge_features_to_test_set():\n",
    "#     ui_set = pd.read_csv('data/test_set/ui_set.csv')\n",
    "#     df_f3_1to4 = pd.read_csv('data/test_set/feature_set_behavior_count.csv')\n",
    "#     df_f3_5to8 = pd.read_csv('data/test_set/feature_set_behavior_lasttime_delta.csv')\n",
    "#     df_f2_1to2 = pd.read_csv('data/test_set/feature_u_count.csv')\n",
    "#     pd.concat([ui_set, df_f3_1to4, df_f3_5to8, df_f2_1to2], axis=1).to_csv('data/test_set/test_set.csv', index=False)\n",
    "    \n",
    "def merge_features_to_test_set1212():\n",
    "#     ui_set = pd.read_csv('data/test_set/1212ui_set.csv')\n",
    "#     df_f3_1to4 = pd.read_csv('data/test_set/1212feature_set_behavior_count.csv')\n",
    "#     df_f3_5to8 = pd.read_csv('data/test_set/1212feature_set_behavior_lasttime_delta.csv')\n",
    "#     df_f2_1to2 = pd.read_csv('data/test_set/1212feature_u_count.csv')\n",
    "#     df_f1 = pd.read_csv('data/test_set/1212feature_i_count.csv')\n",
    "    pd.concat([\n",
    "            pd.read_csv('data/test_set/1212ui_set.csv'),\n",
    "            pd.read_csv('data/test_set/1212feature_set_behavior_count.csv'),\n",
    "            pd.read_csv('data/test_set/1212feature_set_behavior_lasttime_delta.csv'),\n",
    "            pd.read_csv('data/test_set/1212feature_u_count.csv'),\n",
    "            pd.read_csv('data/test_set/1212feature_i_count.csv'),\n",
    "        ], axis=1).to_csv('data/test_set/1212test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_test_UI()\n",
    "\n",
    "## 1212\n",
    "#merge_features_to_test_set1212()\n",
    "\n",
    "# #~1212\n",
    "# merge_features_to_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
